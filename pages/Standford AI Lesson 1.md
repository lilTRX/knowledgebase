- [video of lesson](https://www.youtube.com/watch?v=J8Eh7RqggsU)
- [slides](https://stanford-cs221.github.io/autumn2019/lectures/index.html#include=overview.js&mode=print1pp)
- [homework](https://stanford-cs221.github.io/autumn2019/assignments/foundations/index.html)
- ## Topics
- Overview, but i skipped that as its not important to learn this
- [[Optimization]]
- ## Homework
- ### Problem 1
- a
	- the value of 0 of f(0) minimizes the distance to the 0 punkt
	- taking the half of a negative number makes it bigger not smaller, this would obfuscate the result
- b
	- f(x) is the the "positive sum" of all x so basically flips all x to be positive and then sums them
	- g(x) give us the bigger sum of either all negativly fliped x values or the sum of the x values as is
	- f(x) is always greater but i dont know how to show this with math, the idea is that f(x)-g(x) >=0
- c
	- 1 in 6 to stop
	- 1 in 6 to get a
	- 1 in 6 to get b
	- Expected value = (probability of scoring b points) x (b points) + (probability of losing a point) x (-a points) + (probability of scoring 0 points) x (0 points)
	- Expected value = (1/6) x (b) + (1/6) x (-a) + (4/6) x (0)
	- Expected value = (b-a)/6
- d
	- the derivative of log(L(p)) is maximizing getting head on the first turn based on the obseved data that is provided by the task
- e
	- the *λ* lambad term that is multiplied by the amount of dimensions is a regularization tool.the higher *λ* is, the more it regulates the term, the lower it is the lower is the regulation
	-
	-
- Problem 2
	- we have 6 features we want to identify
	- each feature is in a box with the position n*n and the dimensions n*n
	- this lead to n^4
	- we do this 6 times
	- n^4*6
	- we skip the
	-